{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dec43a44e7324653",
   "metadata": {
    "id": "dec43a44e7324653"
   },
   "source": [
    "# Assignment 2: Vision Transformers & Vision-Language Models\n",
    "\n",
    "The first goal of this assignment is to understand how the transformer architecture introduced in the first assignment can be extended from text to images, resulting in a Vision Transformer (ViT). Then, you will learn more about joint modelling of text and image data, i.e. vision-language models (VLMs). Using Contrastive Language-Image Pretraining (CLIP) as a specific case study of VLMs, you will investigate how these models bridge modalities to enable tasks such as matching images to text \n",
    "prompts. \n",
    "\n",
    "Since VLMs are computationally very demanding to train (even for small datasets), you will work with already pretrained vision–language models based on the OpenCLIP architecture. You will evaluate their ability to associate histopathology images with textual concepts through the task of magnification prediction, which is defined as a binary classification distinguishing between low-power and high-power magnification in microscopy images (Figures 1 and 2). While this task holds limited clinical value (as pathologists do not need assistance identifying magnification), it is relatively simple and sufficient to demonstrate the ability of pre-trained models to encode and distinguish domain-specific visual features in their latent space. Although these categories are visually distinct, models pretrained on non-histopathology image-text pairs may not reliably associate these visual patterns with such domain-specific textual labels.\n",
    "\n",
    "To address cases when the ViT encodes meaningful features of domain-specific images but does not have the concepts explicitly labeled, we will first introduce a linear probing technique. Linear probing involves training a lightweight classifier on top of **frozen** image embeddings to test whether the relevant information is already present in the representation (latent) space of the vision encoder. This technique allows us to use the learned image representation for classification tasks without retraining the vision transformer (which would be computationally expensive).\n",
    "\n",
    "Finally, we will compare the linear probing results with the zero-shot classification performance of two vision transformers with the same architecture but pretrained on different datasets: one on general natural images and one on medical images. Zero-shot classification involves using the pretrained alignment between text and images to classify data into categories that were not explicitly seen during the model's training. \n",
    "\n",
    "As in the previous assignment, we will use the [Open-MELON dataset](https://huggingface.co/datasets/MartiHan/Open-MELON-VL-2.5K) for both of these tasks. \n",
    "\n",
    "![Low-power view example](../assets/low_power_example.png)\n",
    "\n",
    "*Figure 1: Example of low-power (low-magnification) view from Open-MELON dataset*\n",
    "\n",
    "![High-power view example](../assets/high_power_example.png)\n",
    "\n",
    "*Figure 2: Example of high-power (high-magnification) view from Open-MELON dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8230544",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "This is a list of materials that you can use to learn the topic of Transformers and prepare for the flipped classrooms:\n",
    "\n",
    "| Type | Length | Link | Why is it relevant? |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| Paper | 9 pg. (w/o supplementary material)| [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al.)](https://arxiv.org/abs/2010.11929) | This is the paper that introduced the Vision Transformer (ViT) model. ViT uses the encoder part of the original Transformer architecture to process images as sequences of patches. There is no causal masking, meaning that each image patch (i.e. image \"word\") can \"see\"/attend to all other patches in the image. This enables the model to build a global understanding of the complete image. As with the Transformers models for text, positional embedding of the patches is used. Its primary goal is to map the input image into a fixed vector representation (using a special `[CLS]` token) for classification tasks, rather than generating a new sequence of data autoregressively.|\n",
    "| Video | 30 min. | [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Paper Explained)(Yannic Kilcher)](https://www.youtube.com/watch?v=TrdevFK_am4) | This is a very good explanation of the ViT paper. It goes through the key concepts and explains them in a clear and concise manner. You can iterate between reading (parts of) the paper and watching the video to deepen your understanding.|\n",
    "| Paper | 27 pg. (w/o supplementary material) | [Learning Transferable Visual Models From Natural Language Supervision (Radford et al.)](https://arxiv.org/abs/2103.00020) | This paper introduces CLIP (Contrastive Language-Image Pre-training). It uses a Vision Transformer (ViT) as the image encoder and trains it jointly with a text encoder to predict correct image-text pairs. This demonstrates how the fixed vector representation (e.g., from the `[CLS]` token) of a ViT can be aligned with natural language, enabling the model to perform zero-shot classification on entirely new tasks without specific training labels. |\n",
    "| Video | 48 min. | [OpenAI CLIP: Connecting Text and Images (Paper Explained) (Yannic Kilcher)](https://www.youtube.com/watch?v=T9XSU0pKX2E) | This video breaks down the CLIP paper, explaining exactly how the ViT image encoder and text encoder are trained together using contrastive loss. It visually walks through the architecture and the results, making the concept of \"zero-shot transfer\" (using the model on new tasks without fine-tuning) much easier to understand. |\n",
    "| Paper | 10 pg. (w/o supplementary material) |[A visual-language foundation model for computational pathology (Lu et al.)](https://www.nature.com/articles/s41591-024-02856-4)) | This paper presents CONCH, a vision-language foundation model pretrained on 1.17 million histopathology image-caption pairs. It serves as a state-of-the-art example of a VLM applied to the same domain as the assignment dataset (histopathology), illustrating the data scale required for clinical utility. |\n",
    "| Paper | 14 pg. | [A Survey on Multimodal Large Language Models (Yin et al.)](https://arxiv.org/abs/2306.13549) | As the survey paper in the first assignment, this paper is recommended to get the \"big picture\" of VLMs. It provides a comprehensive overview of the current state of the art in multimodal large language models. Again, treat it as \"optional\" reading for the assignment. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5f955b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63ae5f15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6e3e090ec114f37",
   "metadata": {
    "id": "d6e3e090ec114f37"
   },
   "source": [
    "\n",
    "\n",
    "## Exercises: Theory \n",
    "\n",
    "⚠️ *The answers to the theory exercises are NOT to be submitted as part of the assignment deliverables. They can, however, be used to check your understanding of the materials and to prepare for the flipped classroom. You CAN include your discussion about the exercises in the flipped classroom log.*\n",
    "\n",
    "\n",
    "#### Exercise T.1\n",
    "\n",
    "In the standard ViT architecture, a special learnable vector called the `[CLS]` (classification) token is prepended to the sequence of patch embeddings. The final state of this specific token acts as the aggregate representation of the entire image and is fed to the classifier.\n",
    "\n",
    "Assume that you modify the architecture to discard the `[CLS]` token. Instead, you use the final output state corresponding to the top-left patch of the image to drive the classification. Will this \"still work\"? Are there any up- or down-sides to taking this approach?\n",
    "\n",
    "Can you think of or have you encountered any other approaches to aggregate the patch representations into a global image embedding?\n",
    "\n",
    "#### Exercise T.2\n",
    "\n",
    "The input of a ViT is a sequence of vectors, created by flattening patches and multiplying them by a learnable weight matrix $\\textbf{E}$. If a model is pre-trained on ImageNet (standard photographs), the weight matrix $\\textbf{E}$ is shaped to accept patches with 3 color channels (Red, Green, Blue).\n",
    "\n",
    "You want to fine-tune this pre-trained ViT on Multiparametric MRI data of the prostate. To capture the full clinical context, you stack four specific sequences into the channel dimension: T2-weighted, Diffusion-weighted (DWI), Apparent Diffusion Coefficient (ADC), and Dynamic Contrast Enhanced (DCE). Your input images now have 4 channels. Propose some solutions to the dimensionality mismatch between the pre-trained model and your data.\n",
    "\n",
    "#### Exercise T.3\n",
    "\n",
    "In a CNN, the \"receptive field\" of a neuron in the first layer is strictly limited to a small kernel (e.g., 3×3 pixels). It cannot process or relate information outside this region. Is the same limitation present in ViTs? \n",
    "\n",
    "\n",
    "#### Exercise T.4\n",
    "\n",
    "CLIP is trained using a Contrastive Loss function (specifically InfoNCE). For a batch of $N$ image-text pairs, the model maximizes the cosine similarity of the N correct pairings on the diagonal while minimizing the similarity of the $N^2−N$ incorrect pairings.\n",
    "\n",
    "You attempt to train a CLIP model from scratch on a massive dataset, but due to GPU memory constraints, you set the batch size to a very small number (e.g., $N$=8).\n",
    "\n",
    "Why does the model fail to learn meaningful representations, even if you train it for a very long time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff223268053b76e",
   "metadata": {
    "id": "4ff223268053b76e"
   },
   "source": [
    "## Applying a Vision-Language Model\n",
    "\n",
    "### Install Required Packages\n",
    "\n",
    "This notebook uses:\n",
    "- `datasets` (Hugging Face) for dataset loading\n",
    "- `transformers` for pre-trained models\n",
    "- `open_clip_torch` for the OpenCLIP model\n",
    "- `scikit-learn` for metrics and PCA\n",
    "- `pillow` for image manipulation\n",
    "- `matplotlib` for result visualization\n",
    "- `tqdm` for displaying progress bars\n",
    "\n",
    "Let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b19bca961f84cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39b19bca961f84cf",
    "outputId": "891e68b1-f89f-4a7c-cbf6-14287eebc820"
   },
   "outputs": [],
   "source": [
    "!pip install datasets transformers open_clip_torch scikit-learn pillow matplotlib tqdm ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w04qSXiYKOjp",
   "metadata": {
    "id": "w04qSXiYKOjp"
   },
   "source": [
    "### Import the Libraries\n",
    "\n",
    "The following Python modules will be used for ViT models evaluation and visualization of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b651b679ba93860a",
   "metadata": {
    "id": "b651b679ba93860a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import open_clip\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JOvmMmPkLBcX",
   "metadata": {
    "id": "JOvmMmPkLBcX"
   },
   "source": [
    "### Environment Configuration\n",
    "Evaluation of the ViTs may be very slow on CPU-only laptops when applying to a full dataset. Code block below automatically detects whether a GPU is present and applies configuration accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TGmxmW01Ldhu",
   "metadata": {
    "id": "TGmxmW01Ldhu"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f237f903de9aa373",
   "metadata": {
    "id": "f237f903de9aa373"
   },
   "source": [
    "### Load the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43515730d440405",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f43515730d440405",
    "outputId": "c6025934-823d-403c-d75a-1174c3a4a923"
   },
   "outputs": [],
   "source": [
    "ds_train, ds_test = load_dataset(\"MartiHan/Open-MELON-VL-2.5K\", split=[\"train\", \"test\"])\n",
    "print(ds_train, ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3430b3d4acda17",
   "metadata": {
    "id": "5c3430b3d4acda17"
   },
   "source": [
    "### Load the OpenCLIP Models\n",
    "\n",
    "We now download two pretrained OpenCLIP models from HuggingFace Hub, both based on [ViT-B-16](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html) architecture.\n",
    "\n",
    "For a general model, a standard checkpoint `laion2b_s34b_b88k` is selected with default tokenizer. For a model trained on medical image-text pairs we will use [OpenCLIP-BiomedCLIP-Finetuned](https://huggingface.co/mgbam/OpenCLIP-BiomedCLIP-Finetuned)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed4c68afab16101",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "0ad88b6d6136488bba493ad127440669"
     ]
    },
    "id": "fed4c68afab16101",
    "outputId": "cc524954-5315-4f98-eba5-4a9a49b8b7d3"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ViT pretrained on general dataset\n",
    "model_general, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(\"ViT-B-16\", pretrained=\"laion2b_s34b_b88k\", device=device)\n",
    "tokenizer_general = open_clip.get_tokenizer(\"ViT-B-16\")\n",
    "\n",
    "# ViT pretrained on medical dataset\n",
    "model_medical, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(\"hf-hub:mgbam/OpenCLIP-BiomedCLIP-Finetuned\", pretrained=None, device=device)\n",
    "tokenizer_medical = open_clip.get_tokenizer(\"hf-hub:mgbam/OpenCLIP-BiomedCLIP-Finetuned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VpRZKhPOOKW1",
   "metadata": {
    "id": "VpRZKhPOOKW1"
   },
   "source": [
    "### Dataset Wrapper\n",
    "\n",
    "As mentioned, we will utilize the Open-MELON dataset to perform an illustrative classification task predicting image magnification (low vs. high power). The code below is a PyTorch data pipeline that filters the Open-MELON dataset to keep only images with valid magnification metadata and categorizes them into \"low-power\" or \"high-power\". This establishes the ground truth for our task. It also applies necessary image preprocessing transformations and initializes DataLoader instances to efficiently batch and stack these tensors alongside their text labels for use in models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd379e1e5c1bfc94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd379e1e5c1bfc94",
    "outputId": "68ccb631-4087-40f4-f758-5326447a03db"
   },
   "outputs": [],
   "source": [
    "class HFDatasetImages(Dataset):\n",
    "    def __init__(self, hf_ds, preprocess):\n",
    "        self.ds = hf_ds\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "        # keep only samples with known magnification\n",
    "        self.valid_indices = []\n",
    "        for i in range(len(self.ds)):\n",
    "            if self.ds[i].get(\"magnification\", None) is not None:\n",
    "                self.valid_indices.append(i)\n",
    "\n",
    "        print(f\"Kept {len(self.valid_indices)} / {len(self.ds)} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.ds[self.valid_indices[idx]]\n",
    "\n",
    "        img = ex[\"image\"].convert(\"RGB\")\n",
    "        try:\n",
    "            mag_num = int(ex[\"magnification\"])\n",
    "            if mag_num <= 80:\n",
    "                mag_label = \"low-power\"\n",
    "            else:\n",
    "                mag_label = \"high-power\"\n",
    "        except:\n",
    "            mag_label = ex[\"magnification\"]\n",
    "\n",
    "        x_img = self.preprocess(img)\n",
    "        return x_img, mag_label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, labels = zip(*batch)\n",
    "    imgs = torch.stack(imgs, dim=0)\n",
    "    return imgs, list(labels)\n",
    "\n",
    "loader_train = DataLoader(\n",
    "    HFDatasetImages(ds_train, preprocess_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "loader_test = DataLoader(\n",
    "    HFDatasetImages(ds_test, preprocess_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e294b8065a594d0",
   "metadata": {
    "id": "2e294b8065a594d0"
   },
   "source": [
    "### Generate Image Embeddings\n",
    "\n",
    "\n",
    "The two loaded OpenCLIP models will be used to generate embeddings for both training and testing set. In Colab, the evaluation might take up to 30 minutes on the CPU-only session, and approximately 1 minute on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9455d54fca5de9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "af9455d54fca5de9",
    "outputId": "b211b36a-8de3-488b-a42c-8c7c18bb4d69"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_image_embeddings(dataloader, model, device):\n",
    "    img_embs = []\n",
    "    labels = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for imgs, labs in dataloader:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "\n",
    "        feats = model.encode_image(imgs)\n",
    "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        img_embs.append(feats.cpu().numpy())\n",
    "        labels.extend(labs)\n",
    "\n",
    "    return np.vstack(img_embs), labels\n",
    "\n",
    "#### Medical OpenCLIP model ####\n",
    "img_emb_med_train, labels_train = compute_image_embeddings(loader_train, model_medical, device)\n",
    "print(\"Train embeddings:\", img_emb_med_train.shape)\n",
    "\n",
    "img_emb_med_test, labels_test = compute_image_embeddings(loader_test, model_medical, device)\n",
    "print(\"Test embeddings:\", img_emb_med_test.shape)\n",
    "\n",
    "#### General OpenCLIP model ####\n",
    "img_emb_gen_train, labels_train = compute_image_embeddings(loader_train, model_general, device)\n",
    "print(\"Train embeddings:\", img_emb_gen_train.shape)\n",
    "\n",
    "img_emb_gen_test, labels_test = compute_image_embeddings(loader_test, model_general, device)\n",
    "print(\"Test embeddings:\", img_emb_gen_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4223265d5d315e4f",
   "metadata": {
    "id": "4223265d5d315e4f"
   },
   "source": [
    "### Visualizing the Embeddings\n",
    "\n",
    "The produced image embeddings are high-dimensional vectors (512 dimensions) that capture visual information learned by the vision transformer. To better understand how these embeddings are organized, we use Principal Component Analysis (PCA) to project them into a two-dimensional space that can be visualized.\n",
    "\n",
    "PCA is a linear dimensionality reduction technique that finds directions (principal components) along which the data varies the most. When we project the embeddings onto the first two principal components, we preserve as much of the original variance as possible while reducing the dimensionality. Importantly, PCA does not use class labels; it reflects only the structure present in the embeddings themselves.\n",
    "\n",
    "For visualization purposes, we concatenate the training and testing sets and apply PCA to the combined embeddings. Each point in the resulting plot represents one image, positioned according to its embedding in the reduced space. Points that are close together correspond to images with similar embeddings, while points that are far apart indicate greater dissimilarity as perceived by the model.\n",
    "\n",
    "It is important to note that PCA is only a visualization tool: separation in the plot does not directly imply classification performance. However, such plots are valuable for building intuition about what information the model has learned and how different visual factors are reflected in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3528ed61f44c3abc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "3528ed61f44c3abc",
    "outputId": "254d7859-5bd7-4953-fc4d-c8404074e74d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# --- concatenate train + test ---\n",
    "img_emb_med = np.concatenate([img_emb_med_train, img_emb_med_test], axis=0)\n",
    "labels  = np.concatenate([labels_train, labels_test], axis=0)\n",
    "\n",
    "img_emb_gen = np.concatenate([img_emb_gen_train, img_emb_gen_test], axis=0)\n",
    "\n",
    "y_labels = np.array([1 if c == \"high-power\" else 0 for c in labels])\n",
    "\n",
    "# --- PCA projections ---\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "\n",
    "X_med = pca.fit_transform(img_emb_med)\n",
    "X_gen = pca.fit_transform(img_emb_gen)\n",
    "\n",
    "# --- plotting ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=False)\n",
    "\n",
    "# Medical ViT\n",
    "axes[0].scatter(X_med[y_labels == 0, 0], X_med[y_labels == 0, 1],\n",
    "                s=12, alpha=0.7, label=\"low-power\")\n",
    "axes[0].scatter(X_med[y_labels == 1, 0], X_med[y_labels == 1, 1],\n",
    "                s=12, alpha=0.7, label=\"high-power\")\n",
    "axes[0].set_title(\"Medical ViT embeddings\")\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel(\"PC1\")\n",
    "axes[0].set_ylabel(\"PC2\")\n",
    "\n",
    "# General ViT\n",
    "axes[1].scatter(X_gen[y_labels == 0, 0], X_gen[y_labels == 0, 1],\n",
    "                s=12, alpha=0.7, label=\"low-power\")\n",
    "axes[1].scatter(X_gen[y_labels == 1, 0], X_gen[y_labels == 1, 1],\n",
    "                s=12, alpha=0.7, label=\"high-power\")\n",
    "axes[1].set_title(\"General ViT embeddings\")\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel(\"PC1\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IBjC0y4RlOKI",
   "metadata": {
    "id": "IBjC0y4RlOKI"
   },
   "source": [
    "When examining the results, you may observe that embeddings from the medical vision transformer show somewhat clearer separation between low-power and high-power images compared to the general vision transformer. This suggests that magnification-related information is more explicitly encoded in the representations of the medical model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sLM-7u5lKr8u",
   "metadata": {
    "id": "sLM-7u5lKr8u"
   },
   "source": [
    "### Linear Probing\n",
    "To better understand whether magnification information is present in the image embeddings themselves (independently of language alignment) we now apply linear probing. In linear probing, we freeze the image embeddings and train a logistic regression classifier on top of them using labeled data. This classifier learns the optimal linear weights directly from the data, rather than relying on fixed weights derived from text prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc36fc9bc35cbf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6cc36fc9bc35cbf",
    "outputId": "cb03a6d5-1a05-44dc-fc2c-79b2b9001e30"
   },
   "outputs": [],
   "source": [
    "def make_binary_labels(labels):\n",
    "    \"\"\"\n",
    "    Convert string labels to binary:\n",
    "    low-power -> 0\n",
    "    high-power -> 1\n",
    "    \"\"\"\n",
    "    return np.array(\n",
    "        [1 if c == \"high-power\" else 0 for c in labels],\n",
    "        dtype=np.int32\n",
    "    )\n",
    "\n",
    "def linear_probe(\n",
    "    X_train, y_train,\n",
    "    X_test,  y_test,\n",
    "    max_iter=2000,\n",
    "    class_weight=\"balanced\"\n",
    "):\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_std = scaler.fit_transform(X_train)\n",
    "    X_test_std  = scaler.transform(X_test)\n",
    "\n",
    "    # Train logistic regression\n",
    "    clf = LogisticRegression(\n",
    "        max_iter=max_iter,\n",
    "        class_weight=class_weight\n",
    "    )\n",
    "    clf.fit(X_train_std, y_train)\n",
    "\n",
    "    # Probabilities for positive class\n",
    "    y_prob = clf.predict_proba(X_test_std)[:, 1]\n",
    "\n",
    "    # ROC + AUC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    return {\n",
    "        \"model\": clf,\n",
    "        \"scaler\": scaler,\n",
    "        \"y_prob\": y_prob,\n",
    "        \"fpr\": fpr,\n",
    "        \"tpr\": tpr,\n",
    "        \"auc\": auc,\n",
    "    }\n",
    "\n",
    "\n",
    "y_train = make_binary_labels(labels_train)\n",
    "y_test  = make_binary_labels(labels_test)\n",
    "\n",
    "\n",
    "print(\"Class balance:\", np.bincount(y_train))\n",
    "\n",
    "print(\"#### Logistic Regression on Medical ViT ###\")\n",
    "\n",
    "lr_med = linear_probe(\n",
    "    X_train=img_emb_med_train,\n",
    "    y_train=y_train,\n",
    "    X_test=img_emb_med_test,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "print(f\"-AUC score: {lr_med[\"auc\"]:.2f}/1.00\")\n",
    "\n",
    "print(\"#### Logistic Regression on General ViT ###\")\n",
    "\n",
    "lr_gen = linear_probe(\n",
    "    X_train=img_emb_gen_train,\n",
    "    y_train=y_train,\n",
    "    X_test=img_emb_gen_test,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "print(f\"-AUC score: {lr_gen[\"auc\"]:.2f}/1.00\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5K89XUJMotwc",
   "metadata": {
    "id": "5K89XUJMotwc"
   },
   "source": [
    "### Zero-shot Model Evaluation\n",
    "So far, we have only used the image encoder, effectively treating OpenCLIP as a standard vision backbone model. We have not yet made use of the text encoder or the aligned latent space that characterizes Vision-Language Models.\n",
    "\n",
    "One way we can do that is by exploiting the contrastive nature of the pretraining. Because the model was optimized to maximize the similarity between matching image-text pairs, we can classify images by directly comparing their embeddings to the embeddings of potential text labels.\n",
    "\n",
    "In the following code, we define two simple text prompts—\"low-power\" and \"high-power\"—encode them using the text encoder, and compute cosine similarities between the image and text embeddings. The resulting similarity scores are converted into class probabilities using a softmax function, allowing us to evaluate the so called \"zero-shot\" classification performance without training a specific classifier.\n",
    "\n",
    "Unlike linear probing, zero-shot evaluation does not learn any task-specific classifier from labeled data. Instead, it relies entirely on the alignment between image and text embeddings learned during contrastive pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KBFCZeN3w5PW",
   "metadata": {
    "id": "KBFCZeN3w5PW"
   },
   "outputs": [],
   "source": [
    "PROMPTS = [\n",
    "    \"low-power\",\n",
    "    \"high-power\",\n",
    "]\n",
    "\n",
    "@torch.no_grad()\n",
    "def zeroshot_clip_scores(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    img_emb_np,\n",
    "    prompts,\n",
    "    device\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    # --- text embeddings ---\n",
    "    tokens = tokenizer(prompts).to(device)\n",
    "    txt_emb = model.encode_text(tokens)\n",
    "    txt_emb = txt_emb / txt_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # --- image embeddings ---\n",
    "    img_emb = torch.from_numpy(img_emb_np).to(device)\n",
    "    img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # --- similarity & probability ---\n",
    "    logits = img_emb @ txt_emb.T\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "    # probability of \"high magnification\"\n",
    "    return probs[:, 1].detach().cpu().numpy()\n",
    "\n",
    "\n",
    "y = np.array(\n",
    "    [1 if c == \"high-power\" else 0 for c in labels_test],\n",
    "    dtype=np.int32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_8-9ukUwpQ28",
   "metadata": {
    "id": "_8-9ukUwpQ28"
   },
   "source": [
    "### Receiver Operating Characteristic\n",
    "\n",
    "To compare different classification approaches in a threshold-independent way, we use Receiver Operating Characteristic (ROC) curves. An ROC curve plots the true positive rate (sensitivity) against the false positive rate (1 − specificity) as the decision threshold is varied. Instead of committing to a single cutoff (such as probability ≥ 0.5), the ROC curve summarizes model behavior across all possible thresholds.\n",
    "\n",
    "The area under the ROC curve (ROC-AUC) provides a single scalar measure of performance. An AUC of 0.5 corresponds to random guessing, while an AUC of 1.0 indicates perfect separation. Importantly, ROC-AUC measures how well a model ranks positive samples above negative ones, which makes it particularly suitable for comparing models with different calibration or decision rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318fc1b0d2301366",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 757
    },
    "id": "318fc1b0d2301366",
    "outputId": "e7827b91-d267-4d28-9014-e76cedffabbf"
   },
   "outputs": [],
   "source": [
    "# --- General ViT ---\n",
    "p_high_gen = zeroshot_clip_scores(\n",
    "    model_general,\n",
    "    tokenizer_general,\n",
    "    img_emb_gen_test,\n",
    "    PROMPTS,\n",
    "    device\n",
    ")\n",
    "\n",
    "auc_gen = roc_auc_score(y, p_high_gen)\n",
    "fpr_gen, tpr_gen, _ = roc_curve(y, p_high_gen)\n",
    "\n",
    "print(f\"General ViT zero-shot ROC-AUC: {auc_gen:.3f}\")\n",
    "print(f\"General ViT logistic regression ROC-AUC: {lr_gen['auc']:.3f}\")\n",
    "\n",
    "# --- Medical ViT ---\n",
    "p_high_med = zeroshot_clip_scores(\n",
    "    model_medical,\n",
    "    tokenizer_medical,\n",
    "    img_emb_med_test,\n",
    "    PROMPTS,\n",
    "    device\n",
    ")\n",
    "\n",
    "auc_med = roc_auc_score(y, p_high_med)\n",
    "fpr_med, tpr_med, _ = roc_curve(y, p_high_med)\n",
    "\n",
    "\n",
    "print(f\"Medical ViT zero-shot ROC-AUC: {auc_med:.3f}\")\n",
    "print(f\"Medical ViT logistic regression ROC-AUC: {lr_med['auc']:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(6.8, 6.8))\n",
    "\n",
    "# Logistic regression probes\n",
    "plt.plot(lr_med[\"fpr\"], lr_med[\"tpr\"], lw=2,\n",
    "         label=f\"LR probe — Medical ViT (AUC={lr_med['auc']:.3f})\")\n",
    "\n",
    "# Zero-shot CLIP\n",
    "plt.plot(fpr_med, tpr_med, lw=2,\n",
    "         label=f\"Zero-shot CLIP — Medical ViT (AUC={auc_med:.3f})\")\n",
    "\n",
    "plt.plot(lr_gen[\"fpr\"], lr_gen[\"tpr\"], lw=2,\n",
    "         label=f\"LR probe — General ViT (AUC={lr_gen['auc']:.3f})\")\n",
    "\n",
    "plt.plot(fpr_gen, tpr_gen, lw=2,\n",
    "         label=f\"Zero-shot CLIP — General ViT (AUC={auc_gen:.3f})\")\n",
    "\n",
    "# Chance line\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Chance\")\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Magnification classification — ROC comparison\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yI-a3YMWd8mc",
   "metadata": {
    "id": "yI-a3YMWd8mc"
   },
   "source": [
    "⚠️ *Everything below this line must be submitted as a deliverable for this assignment.*\n",
    "\n",
    "## Exercises: Practice\n",
    "\n",
    "#### Exercise 1\n",
    "Replace the lightweight logistic regression classifier in the linear probing example with custom deep neural network architecture. Justify the model and training hyperparameters and comment on the improvements in the classification performance compared to the linear probing.\n",
    "\n",
    "#### Exercise 2 \n",
    "\n",
    "Inspect the content of [Open-MELON](https://huggingface.co/datasets/MartiHan/Open-MELON-VL-2.5K) dataset. Propose 3 possible classification tasks other than predicting the magnification level. Implement the classification using the zero-shot method and qualitatively evaluate the performance.\n",
    "\n",
    "#### Exercise 3\n",
    "\n",
    "One of the key capabilities of CLIP models is the ability to compare images based on their semantic content rather than low-level pixel similarity.\n",
    "\n",
    "Select 3 query images and use them to search for similar images within the dataset. Image–image retrieval can be performed as follows:\n",
    "\n",
    "1. Encode the query image into an embedding using the medical model’s image encoder.\n",
    "\n",
    "2. Compute the cosine distance between the query embedding and the precomputed image embeddings of the dataset.\n",
    "\n",
    "3. Retrieve and visualize the top 5 most similar images for each query image (excluding the query image itself).\n",
    "\n",
    "Do the retrieved images visually correspond to the query image? Comment on which visual or semantic properties (e.g. tissue structure, staining patterns, or layout) appear to drive the similarity.\n",
    "\n",
    "#### Exercise 4\n",
    "\n",
    "ViTs use self-attention to model interactions between image patches. In each attention layer, tokens exchange information with one another, gradually forming a global image representation. In this assignment, we visualize patch → CLS attention, which shows how strongly each image patch contributes to the CLS token at a given layer.\n",
    "\n",
    "A utility for extracting and visualizing attention maps is provided in `code/vit_attention_utils.py`. Run the setup cell below to access the utility. If you are working in Google Colab and have uploaded only this notebook, the course repository content will be cloned automatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8b63c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "if running_in_colab():\n",
    "  if not os.path.isdir('project-ai-medical-imaging'):\n",
    "      !git clone https://github.com/tueimage/project-ai-medical-imaging.git\n",
    "  !ls\n",
    "  %cd /content/project-ai-medical-imaging/code\n",
    "else:\n",
    "  %cd ../code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6e9b92",
   "metadata": {},
   "source": [
    "After executing the code cell below, attention maps are displayed for 4 test images for both the medical and general ViT models. The interactive sliders allow you to navigate across transformer layers and inspect individual attention heads. Setting the head slider to -1 displays the attention averaged across all heads in the selected layer. Different test images can be visualized by modifying the indices in the `idxs` array (valid range: 0–397)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ea444f0f33b1b7",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from vit_attention_utils import ViTAttentionViewer\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "viewer = ViTAttentionViewer(\n",
    "    model_medical=model_medical,\n",
    "    model_general=model_general,\n",
    "    preprocess_fn=preprocess_val,\n",
    "    device=device,\n",
    "    mean=preprocess_val.transforms[-1].mean,\n",
    "    std=preprocess_val.transforms[-1].std,\n",
    ")\n",
    "\n",
    "idxs = [0, 10, 100, 102]\n",
    "\n",
    "num_heads = model_medical.visual.trunk.blocks[0].attn.num_heads\n",
    "\n",
    "@interact(\n",
    "    layer=IntSlider(min=0, max=11, step=1, value=0),\n",
    "    head=IntSlider(min=-1, max=num_heads-1, step=1, value=-1),\n",
    ")\n",
    "def view(layer, head):\n",
    "    head_sel = \"mean\" if head == -1 else head\n",
    "    viewer.show_side_by_side(\n",
    "        ds_test,\n",
    "        idxs,\n",
    "        layer=layer,\n",
    "        head=head_sel,\n",
    "        alpha=0.5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781bd68af1243963",
   "metadata": {},
   "source": [
    "Your task is to comment on the differences between the attention maps of the general and medical models on at least 4 images of your choice. You can, for example, select images with prominent figure labels to examine how each model attends to textual regions, or images with heterogeneous tissue architecture to explore which visual patterns receive the most attention. Images originating from the same paper appear at consecutive indices in the dataset, so selecting nearby indices may yield visually similar examples.\n",
    "\n",
    "Describe how the attention patterns change across layers and how different heads behave within a layer. Focus on what parts of the images appear important to each model and how this evolves through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7339a488",
   "metadata": {},
   "source": [
    "## Flipped Classroom Log\n",
    "\n",
    "ℹ️ *You have to fill this log for both flipped classroom sessions for this assignment. You only fill the log for your group, not together with the group that you interacted with.*\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Preparation\n",
    "* **Clients:** List specific theoretical or implementation (code) questions prepared before class.\n",
    "* **Consultants:** List papers, videos, or code documentation reviewed to prepare. Note, this is not limited to the material listed above, you can add any new material that you used or found useful. \n",
    "\n",
    "#### Peer Interaction\n",
    "* **Clients:** Summarize the solutions or explanations received.\n",
    "* **Consultants:** Summarize advice given and specific resources shared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d0d5c",
   "metadata": {},
   "source": [
    "### Logs\n",
    "#### First Flipped Classroom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0bed1",
   "metadata": {},
   "source": [
    "\n",
    "ℹ️ *Write the log for your group in this cell. Should be in a narrative style, aim for a max. of 600 words.*\n",
    "\n",
    "**Role**: Client/Consultant\n",
    "\n",
    "**Description** of activities during flipped classroom:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ba6cad",
   "metadata": {},
   "source": [
    "#### Second Flipped Classroom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9a7f47",
   "metadata": {},
   "source": [
    "ℹ️ *Write the log for your group in this cell. Should be in a narrative style, aim for a max. of 600 words.*\n",
    "\n",
    "**Role**: Client/Consultant\n",
    "\n",
    "**Description** of activities during flipped classroom:"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
