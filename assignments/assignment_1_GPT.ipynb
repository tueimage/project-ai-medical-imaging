{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5462d987",
   "metadata": {
    "id": "5462d987"
   },
   "source": [
    "# Assignment 1: Generative Pretrained Transformer (GPT)\n",
    "\n",
    "The goal of this assignment is to master the **Transformer architecture**, which is the engine behind modern Large Language Models (LLMs) such as ChatGPT, Gemini, and DeepSeek. We will implement a compact, decoder-only transformer from scratch, closely following the [NanoGPT](https://github.com/karpathy/nanoGPT) project by Andrej Karpathy.\n",
    "\n",
    "We will train this model on textual figure captions from the [Open-MELON dataset](https://huggingface.co/datasets/MartiHan/Open-MELON-VL-2.5K), which contains descriptions of histopathology images (specifically melanocytic lesions). After training, your model will be able to generate **synthetic captions** when prompted with a short starting sequence. Note that at this point we do not associate these captions with image data. We will go into vision-language models that can jointly model both image and text data in the next assignment. For now, we focus on text, but, as you will see in Assignment 2, extending the Transformer architecture to model image data is a straightforward extension. \n",
    "\n",
    "This dataset was specifically prepared for this course (work done by Martina Hanusova). While this dataset is certainly not as large as the massive corpora used to train ChatGPT and consists of open-access figures and captions rather than medical images and clinical reports, it offers two  advantages. First, the content is highly dense with relevant medical terminology, which makes the training process efficient. Second, because we are using public data from medical publications, we avoid the complex legal and ethical hurdles associated with handling private patient data.\n",
    "\n",
    "## Preparation\n",
    "\n",
    "We start with the assumption that you have a solid understanding of fundamental machine learning and neural network concepts, such as linear regression, gradient descent, backpropagation, and loss functions; if you need a refresher, please review materials from the previous courses (e.g. 8BB020 Introduction to Machine Learning). You must prepare for this assignment by mastering the Transformer architecture, specifically understanding self-attention mechanisms in neural networks and the distinction between encoder models and the decoder-only architectures (like GPT) used for generative tasks.\n",
    "\n",
    "Beyond the architecture, you must also familiarize yourself with the basics of Computational Pathology to understand the medical context of our dataset (we will be generating captions for H&E stained histopathology images).\n",
    "\n",
    "This is a list of materials that you can use to learn the topic of Transformers and prepare for the flipped classrooms:\n",
    "\n",
    "| Type | Length | Link | Why is it relevant? |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| Video | 8 min. | [Large Language Models explained briefly (3Blue1Brown)](https://www.youtube.com/watch?v=LPZh9BOjkQs&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=5) | This video from the [3Blue1Brown](https://www.3blue1brown.com/) YouTube channel was originally made as an exhibit for Computer History Museum. It gives a very \"gentle\" and high-level introduction of the mechanics of how (large) language models work and are trained. This includes the concept of autoregressive modelling (predicting the next word in a sequence), (pre)-training of large language models from text datasets (the \"P\" in GPT stands for \"pretrained\") as well as reinforcement learning with human feedback. While in this course we will keep to training of (relatively small) language models, it is good to have the complete picture.  The entire 3Blue1Brown channel is a very high-quality source of educational content on a wide range of topics. In fact, the entire [Neural networks]() playlist is highly recommended, also as a refresher to more fundamental concepts such as backpropagation. | \n",
    "| Paper | 10 pg. | [Attention Is All You Need (Vaswani et al.)](https://arxiv.org/abs/1706.03762) | This is the paper that introduced the Transformer architecture to the world. It is a must-read for anyone interested in the subject. The paper is relatively short (10 pages without references), however it can be a bit \"dense\" for beginners. That is ok, as the most important thing to understand is the tokenization of text and the self-attention mechanism. Both of these concepts are also covered in the next two recommended videos so it is best that you iterate between watching the videos and reading the paper. Note that one thing that might be particularly confusing is that the neural network architecture in this paper has both an encoder and decoder part. This is because the application that is addressed here is natural language translation (machine translation). In this setup, the encoder part is used to encode the text in the original language and the decoder is used to translate this encoded text into the target language. In this course we will focus on decoder-only architectures, which are used for generative tasks such as text generation. | \n",
    "| Video | 27 min. | [Transformers, the tech behind LLMs (3Blue1Brown)](https://www.youtube.com/watch?v=wjZofJX0v4M&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) | This video provides a visual overview of the data flow within a Transformer, specifically focusing on GPT-style (decoder-only) models. It explains tokenization, embeddings (how vectors encode semantic meaning), and the final softmax layer used to predict the next token. It serves as a high-level roadmap of the architecture before diving into specific components in the next video. |\n",
    "| Video | 26 min. | [Attention in transformers (3Blue1Brown)](https://www.youtube.com/watch?v=eMlx5fFNoYc&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) |This chapter breaks down the Attention mechanism (Self-Attention), which is the core innovation of the Transformer. It details the Query, Key, and Value (Q, K, V) matrices and visualizes how the attention pattern is calculated via dot products. It explains how the model uses these to \"attend\" to relevant context (updating word meanings based on surrounding words) and introduces concepts like multi-head attention and masking. |\n",
    "| Interactive tool | ~25 min. | [Transformer explainer (poloclub)](https://poloclub.github.io/transformer-explainer/)| This is a great interactive tool that explains in steps how transformer models for language work and make predictions. It should take around 25 minutes to go over all the steps in the tool for one example, but it might be useful to spend some more time and look at different examples. | \n",
    "| Video | 120 min. | [Let's build GPT: from scratch, in code, spelled out (Andrej Karpathy)](https://www.youtube.com/watch?v=kCc8FmEb1nY) | This video series provides a step-by-step guide to building a GPT-style transformer from scratch. It covers the entire process, from data preparation to model training and evaluation. It is a great resource for understanding the mechanics of how GPT-style models work and how to implement them in code. It is **optional** in a sense that it is not required for the exercises, but might be very useful for understanding the dataset and **formulating an research question for the open assignment.** |\n",
    "| Paper | 11 pg. | [From melanocytes to melanomas (Shain et al.)](https://www.nature.com/articles/nrc.2016.37) | This review paper provides a good overview of the biology of melanocytic lesions and the histopathology of melanocytic lesions. It is a good resource to understand the medical context of our dataset.|\n",
    "| Paper | 35 pg. | [Large Language Models: A Survey (Minaee et al.)](https://arxiv.org/abs/2402.06196) | This survey paper is a good resource to get the \"big picture\" of various large language models and how they differ in their methodology and implementation. Treat this resource as **optional**. It is not required to complete the exercises in this assignments, but it is very useful to get a broader understanding of the state-of-the-art, which you might find useful for **generating ideas for the open assignment** or future projects (e.g. for your BEP or MSc projects). |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da3c198",
   "metadata": {
    "id": "4da3c198"
   },
   "source": [
    "## Exercises: Theory\n",
    "\n",
    "⚠️ *The answers to the theory exercises are NOT to be submitted as part of the assignment deliverables. They can, however, be used to check your understanding of the materials and to prepare for the flipped classroom. You CAN include your discussion about the exercises in the flipped classroom log.*\n",
    "\n",
    "#### Exercise T.1 \n",
    "\n",
    "A GPT model is autoregressive, meaning it generates text strictly one token at a time. It predicts the first next token, appends it to the sequence, and uses that updated sequence to predict the second token. This is a serial process.\n",
    "\n",
    "In contrast, many other neural networks process their entire input in parallel (all at once) to maximize speed.\n",
    "\n",
    "If we modified the architecture to predict every token in a paragraph simultaneously (in parallel) rather than one by one, is the resulting text likely to be grammatically coherent? Explain your reasoning.\n",
    "\n",
    "#### Exercise T.2\n",
    "\n",
    "In this assignment (as you will see later), we define tokens to be individual characters (e.g., 'a', 'b', 'c'). This keeps our vocabulary small and the implementation simple. In contrast, the standard practice for modern Large Language Models is to use sub-word tokenization, which groups frequent character patterns into single tokens to process text more efficiently. \n",
    "\n",
    "Assume that we take this concept one step further and define tokens to be entire sentences. What would be the consequence of this when generating text?\n",
    "\n",
    "#### Exercise T.3\n",
    "\n",
    "The core concept of a Transformer is self-attention, which allows a token to look at other tokens to gather context. In a Decoder-only model (like GPT), we apply a specific constraint called Causal Self-Attention (or \"Masked\" Self-Attention) during training. This forces every token to look only at itself and the tokens that came before it.\n",
    "\n",
    "This is different from Encoder models, which use Bidirectional Attention, allowing tokens to see the entire sentence (past and future) at once.\n",
    "\n",
    "Assume that during training, we feed the entire completed sentence into the model at once. Will this model still be useful for generating text? Explain your reasoning. \n",
    "\n",
    "#### Exercise T.4 \n",
    "\n",
    "Decoder-only models (like GPT) are mathematically optimized to maximize the probability of the text sequence. This means that if they were trained with medical report data, they strive to generate text that \"looks\" and \"sounds\" like a real medical report. They are not optimized to check for factual truth.\n",
    "\n",
    "Assume that your trained model generates a synthetic pathology caption that includes specific quantitative details: \"The sample shows a high mitotic rate of 8 figures per 10 HPF.\"\n",
    "\n",
    "Why is this specific type of error (a plausible-sounding number) significantly more dangerous to a clinician than if the model had just generated obvious gibberish (e.g., \"rate figures high 8 10\" )?\n",
    "\n",
    "#### Exercise T.5 \n",
    "\n",
    "If we want to train a model that generates a pathology report based on medical imaging data, we would need a joint vision-language model. We will touch on this in the next assignment. For now, assume that we want to generate a report based on some medical test or observations that we can represent as text data and make them part of the starting sequence that we use to prompt further generation.\n",
    "\n",
    "Consider the following example of how such a model would operate:\n",
    "\n",
    "Input Sequence (Provided by User): \"Clinical Indication: Patient has a rapidly growing, asymmetrical black lesion on the left shoulder. Rule out Melanoma. Gross Description: ...\"\n",
    "\n",
    "Generated Continuation (Predicted by Model): \"...Specimen consists of a skin ellipse measuring 2.0 x 1.5 cm. Microscopic examination reveals a proliferation of atypical melanocytes arranged in nests at the dermo-epidermal junction. There is upward migration of cells (pagetoid spread). DIAGNOSIS: MALIGNANT MELANOMA.\"\n",
    "\n",
    "Would such a model be useful in clinical practice? Motivate your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db3a522",
   "metadata": {},
   "source": [
    "\n",
    "## Before you Begin...\n",
    "\n",
    "### Dataset\n",
    "\n",
    "[Open-MELON-VL-2.5K](https://huggingface.co/datasets/MartiHan/Open-MELON-VL-2.5K) is a vision–language dataset of 2,500 Hematoxylin & Eosin-stained histopathology images of melanocytic lesions with captions, curated from PubMed open-access publications. It is hosted on Hugging Face, which is a platform for sharing and accessing datasets.\n",
    "\n",
    "### GPU Usage\n",
    "\n",
    "For the assignments, we recommend to use GPU for faster model training or evaluation.\n",
    "[Google Colab](https://colab.research.google.com) provides a free service for hosting Jupyter notebooks allowing to access a remote Tesla T4 GPU.\n",
    "Please keep in mind the Colab free tier GPU usage limits (approximately 1.5 hours per day, but may be higher or lower depending on your usage history).\n",
    "\n",
    "To enable a GPU in Colab, navigate to the menu panel -> `Runtime` -> `Change runtime type` -> `T4 GPU` -> `Save`.\n",
    "When you are done with your work, detach the current runtime session and download the Jupyter notebook file locally.\n",
    "\n",
    "⚠️ *Note that all assignments are designed to also be doable on the CPUs of your laptops, so you do not have to use Colab or a GPU.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9138fddf",
   "metadata": {
    "id": "9138fddf"
   },
   "source": [
    "## Training a Generative Language Model\n",
    "\n",
    "### Install Required Packages\n",
    "\n",
    "This notebook uses:\n",
    "- `numpy` for basic math operations\n",
    "- `datasets` to load captions from Hugging Face\n",
    "- PyTorch to implement NanoGPT-like model + training loop\n",
    "- `tqdm` for displaying progress bars\n",
    "\n",
    "All other used packages should already be available on your system. If you need help with setting up your Python environment, please ask help from your assigned TA.\n",
    "\n",
    "Let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3QTZmANKVTuk",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T18:00:52.379987Z",
     "start_time": "2025-12-27T18:00:47.099308Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3QTZmANKVTuk",
    "outputId": "a14364cc-6c55-48c2-be86-d9bc7a7b4597"
   },
   "outputs": [],
   "source": [
    "!pip install numpy datasets torch tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ssVxV212Wqoj",
   "metadata": {
    "id": "ssVxV212Wqoj"
   },
   "source": [
    "### Import the Libraries\n",
    "\n",
    "The following Python modules will be used for our NanoGPT implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d549cc2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T18:01:06.703245Z",
     "start_time": "2025-12-27T18:01:01.446523Z"
    },
    "id": "d549cc2c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a72b624",
   "metadata": {
    "id": "3a72b624"
   },
   "source": [
    "### Load the Dataset\n",
    "\n",
    "In the code block below, [Open-MELON dataset](https://huggingface.co/datasets/MartiHan/Open-MELON-VL-2.5K) is loaded using HuggingFace loaders and the figure captions are concatenated into one big training corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55323366",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T18:02:17.163181Z",
     "start_time": "2025-12-27T18:01:10.705229Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416,
     "referenced_widgets": [
      "ad21d04537234271a09a433c3530a941",
      "1fd588dfeff741fcaad2e341dd110377",
      "e2f7418aeee24478b8eb48a1698f2221",
      "1a3386d14d6a492ebd234ff865d44b88",
      "9b1ec6102dff42168fde4041f8d9836d",
      "7f1a4ca104454d6dbcb79acf38bc5751",
      "28b2e23b225746049a7ef29a61abed7c",
      "50388599eff649d1aa8bbd51deb041a8",
      "3fb5baa6aa4b44708d3c59630c75f309",
      "888cf872bd0b488e8f11a1f8bdc47e6b",
      "f2b4a10613a34694b2af8610a6953c87",
      "c56e3a28a6184251956082bb420ec342",
      "35b5cceb485d4170a954407296598dca",
      "52230378af04483e86492961e32e7437",
      "ef5cb6b6696e46478cb1d4e77e2d3bfe",
      "35dc65dee7d24b5ca1fa0f284391b092",
      "6a998639720941af8a88f7f5a727cd5b",
      "b74569efc6c74089be4bd338ac76493f",
      "1f26ab6c3bbe42e59cc3d95bf3777ced",
      "3fd56d0b4372477987cc3c22da161251",
      "ed21b5ed86b24d47afc3583e6c36f48c",
      "7758f5e0dbf541eab4775d9994954db4",
      "4e35834b2e1a41ca93edc66a415234b5",
      "aa99148f96fb4127bb03773943f5ca2c",
      "758f1e6c02b0484c94c5760ac79791b2",
      "1763fc7786a1449d98b2a453d4bd42ac",
      "33e068432a94402ca7e549863f09e216",
      "2c26047212184e259a646453f4afc27f",
      "a582c33ad24f4fbb81da610bc2bb0669",
      "ab42e638f5684fd9908ce1f2917ca126",
      "dcb91c2a326c43e29a29d2d1df3efd1a",
      "c7c8d90c200248e796ed1505513b79e7",
      "bc44d509ebe04821b9693c46ad466ad6",
      "1772b2d0d02a4b20bc7d93f0b437ee87",
      "4fb50a80d6c542d7b9b6626561162389",
      "276b0fa166734df9b630adbd3b3bf7c4",
      "9e3f30127a10406bbf818d3887b7fb15",
      "5cfe1da615004b678338732f4a764e53",
      "d4878509b5a44817a83015e7eaf7ae88",
      "90c6e10716274de2a5ffe74e5d78c998",
      "d1685797aa8a4c16bb4422d0306a7ab1",
      "303e62b5744a43cc8e6c7d6e4b30dec4",
      "7146dc5b01f248898b8a7695c9f773b3",
      "d4ce1ad8fa51402ebde83b933c9fb22e",
      "f47f1baa71b949259a5f3f47cc53e2b2",
      "eba34e6c10ee49ffae22c499b7c8bc46",
      "d8aa861d8c8b484f953fb442c2070985",
      "4f01a807b9d94fc1ad9305478d3b2ff6",
      "4c11875695a941c8bb1228b064c29852",
      "b817c37145a844c3a22f783083a29eca",
      "5563c6c50cda4a7b99e1d2eb6143d717",
      "38430d290ec44789a1c365b926ca6c10",
      "b15c6618e9cb425bab1bfad2f4df50b9",
      "10f54afabb3a475f980868d1ec6fe2db",
      "d202f8ee22a64540ae14e0d1aaca5b06",
      "4420088d42ae489bbaf71e923fb560b2",
      "010c26376ec245b8b50b9838094aebb0",
      "61c175f6209b4367a2087749aa8b24e6",
      "8eda8ca0feb34c3cbcd253a759853f54",
      "b4e00e3f960c433e9735a70ee0cb5d1d",
      "0c21d41600ad432aaf56b33aac43beaa",
      "78b1733d5c4042069ef3c79c0a469d94",
      "52a2fe602f3d4985848cd3652ccc7007",
      "9e8add149e5f4ec79b1819d6b772c314",
      "fecb2c3287e94b17bc419263c5febb66",
      "d044330fd5ac4e549ac49d4b2eccde07",
      "d269950be2b741bc89518fb534263ce1",
      "ab4316c3ecea4fea8945508175c65600",
      "4a7dc64d5d8e41d88fc837accbc9fc7d",
      "f9d3842eda984593838e18ed5fac48fe",
      "4abd8e320472464c8737fcac196e658f",
      "273a3f5ad93b4f328df8a4d356498083",
      "7cbb01e5c45f4637b049990bcff4e095",
      "52b82d42eca2457c88798aa58806a912",
      "ff81c231c6a74c0b8bcb5b7fa40734ad",
      "af495b5536fd47d492903dfe33c2c3a3",
      "d42ca0c3bd9f4a9195aedaac4fafb4f8",
      "8035b0fa1255463b85b8d4350fbbda26",
      "5d01b81ea98140b588f888c17dcd7928",
      "1575bffb712a487e9918cadca9b8dc21",
      "436f0feef68b4cccba44945231e2d9ab",
      "37b980eea0d945d59d3b2fb8c191d899",
      "9a99068517c84bcab970f5a44d2c1286",
      "76844b56c80c4a788fc7aef754f31b60",
      "0c66766c4836440f84fa9e33850a0aa2",
      "e61aac3fa1384ec2a63517d0d8b6e7ba",
      "1603d8d35eb248fc9f0c85db4397d2fc",
      "2086cd42bb22465ab7cf74c4ed9c55e7"
     ]
    },
    "id": "55323366",
    "outputId": "50f5bfbf-7862-4951-f832-544a1c4ce217"
   },
   "outputs": [],
   "source": [
    "# ds_dict is a list of ['train', 'validation', 'test'] splits\n",
    "ds_dict = load_dataset(\"MartiHan/Open-MELON-VL-2.5K\")\n",
    "\n",
    "# in this example, all 3 splits are concatenated\n",
    "# validation set is later drawn from the text chunks of 'block_size'\n",
    "ds_all = concatenate_datasets(list(ds_dict.values()))\n",
    "\n",
    "captions = [str(x) for x in ds_all[\"caption\"]]\n",
    "print(\"Captions:\", len(captions))\n",
    "print(\"Example caption:\", captions[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480f1b8e",
   "metadata": {
    "id": "480f1b8e"
   },
   "source": [
    "#### Preprocess the Training Text\n",
    "\n",
    "We join separate captions with `<ENDC>` separator. This helps the model learn boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83044900",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T18:02:34.787321Z",
     "start_time": "2025-12-27T18:02:34.777342Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83044900",
    "outputId": "109d06f4-1902-4647-96ee-f1e1a6629592"
   },
   "outputs": [],
   "source": [
    "SEP = \"\\n<ENDC>\\n\"\n",
    "text = SEP.join(captions)\n",
    "\n",
    "# Print the total number of characters in the dataset\n",
    "print(\"Training text length (chars):\", len(text))\n",
    "\n",
    "# Print the first 1000 characters of the constructed text corpus\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65a82e7",
   "metadata": {
    "id": "f65a82e7"
   },
   "source": [
    "### Character-level Tokenizer\n",
    "\n",
    "We build a vocabulary of unique characters from the training text. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f4cda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T18:02:42.391469Z",
     "start_time": "2025-12-27T18:02:42.362840Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b0f4cda",
    "outputId": "2114b9c4-85f1-4291-c90b-143ad72cc77c"
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# mapping of characters to numerical tokens (by their order in vocabulary alphabet)\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "\n",
    "# mapping of numerical tokens back to characters\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "def encode(s: str):\n",
    "    \"\"\"\n",
    "    Converts character to a token.\n",
    "    Input: character (e.g. 'A')\n",
    "    Output: numerical token (e.g. 65)\n",
    "    \"\"\"\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(ids):\n",
    "    \"\"\"\n",
    "    Converts token to a character.\n",
    "    Input: numerical token (e.g. 65)\n",
    "    Output: character (e.g. 'A')\n",
    "    \"\"\"\n",
    "    return \"\".join(itos[i] for i in ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8e373",
   "metadata": {},
   "source": [
    "Let's print some information about the vocabulary that we have created as well as some examples of encoding words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7547580",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of the vocabulary:\", vocab_size)\n",
    "print(\"Preview of the vocabulary:\", chars)\n",
    "\n",
    "examples = [\"male\", \"malignant\", \"melanoma\", \"malignant melanoma\"]\n",
    "\n",
    "print(\"\\n--- Encoding Examples ---\")\n",
    "\n",
    "for word in examples:\n",
    "    tokens = encode(word)\n",
    "    \n",
    "    # Create a visual mapping of Char -> Token\n",
    "    mapping_str = \", \".join([f\"'{c}':{t}\" for c, t in zip(word, tokens)])\n",
    "    \n",
    "    print(f\"String:  {word}\")\n",
    "    print(f\"Tokens:  {tokens}\")\n",
    "    print(f\"Mapping: {mapping_str}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa4771",
   "metadata": {},
   "source": [
    "Now we encode the text corpus and store it as PyTorch array. The first 90% of this array will be used for training, while the remaining 10% will be used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9a31d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split: str):\n",
    "    src = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(src) - cfg.block_size - 1, (cfg.batch_size,))\n",
    "    x = torch.stack([src[i:i+cfg.block_size] for i in ix])\n",
    "    y = torch.stack([src[i+1:i+cfg.block_size+1] for i in ix])\n",
    "    return x.to(cfg.device), y.to(cfg.device)\n",
    "\n",
    "print(\"Train tokens:\", train_data.numel(), \"Val tokens:\", val_data.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3b761c",
   "metadata": {
    "id": "ea3b761c"
   },
   "source": [
    "### Model Training Configurations\n",
    "\n",
    "The following configuration classes define the size of the transformer model and how it is trained. These hyperparameters control model capacity, context length, training stability, and computational cost. You can later use this to perform different experiments, e.g. comparing models of different size (capacity) and context length. \n",
    "\n",
    "**Model configuration**\n",
    "\n",
    "- `vocab_size` - number of unique tokens the model can process. In this assignment, tokens are individual characters, so the vocabulary size equals the number of distinct characters in the training corpus.\n",
    "\n",
    "- `block_size` - the context window length, i.e. the maximum number of tokens the model can see at once. During training and generation, the model predicts the next token using only the previous block_size tokens. A larger block size allows the model to capture longer-range dependencies but increases memory and computation requirements.\n",
    "\n",
    "- `n_layer` - number of stacked transformer decoder blocks. More layers increase model depth and expressiveness but also training time and risk of overfitting.\n",
    "\n",
    "- `n_head` - number of attention heads in each self-attention layer. Multiple heads allow the model to attend to different aspects of the context simultaneously (e.g. syntax, formatting, or local patterns).\n",
    "\n",
    "- `n_embd` - dimensionality of token embeddings and hidden representations. Larger embeddings allow richer representations but increase memory usage and compute cost.\n",
    "\n",
    "- `dropout` - dropout probability used during training as a regularization technique. It helps prevent overfitting by randomly deactivating neurons. Dropout is disabled for CPU training to keep behavior deterministic and training stable.\n",
    "\n",
    "**Training configuration**\n",
    "- `batch_size` - Number of training sequences processed in parallel during one optimization step. Larger batches improve gradient stability but require more memory.\n",
    "\n",
    "- `max_iters` - total number of training iterations (parameter update steps).\n",
    "\n",
    "- `eval_interval` - number of training iterations between evaluations on the validation set.\n",
    "\n",
    "- `eval_iters` - number of mini-batches used to estimate training and validation loss during evaluation.\n",
    "\n",
    "- `lr (learning rate)` - step size used by the optimizer when updating model parameters. This is one of the most sensitive hyperparameters for training stability.\n",
    "\n",
    "- `weight_decay` - regularization term that penalizes large weights and helps reduce overfitting.\n",
    "\n",
    "- `device` - specifies whether training runs on CPU or GPU. Smaller model and batch sizes are used automatically when no GPU is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb2940b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T18:02:59.466533Z",
     "start_time": "2025-12-27T18:02:59.160335Z"
    },
    "id": "cfb2940b"
   },
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "### Model configuration (number of layers, number of heads, embedding dimensions, dropout) ###\n",
    "##############################################################################################\n",
    "\n",
    "# Configuration for GPU\n",
    "@dataclass\n",
    "class ModelConfigGPU:\n",
    "    vocab_size: int\n",
    "    block_size: int\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 4\n",
    "    n_embd: int = 256\n",
    "    dropout: float = 0.2\n",
    "\n",
    "# Configuration for CPU\n",
    "@dataclass\n",
    "class ModelConfigCPU:\n",
    "    vocab_size: int\n",
    "    block_size: int\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 4\n",
    "    n_embd: int = 128\n",
    "    dropout: float = 0\n",
    "\n",
    "########################################################################################################\n",
    "### Model configuration (block size, batch size, evaluation iterations, learning rate, weight decay) ###\n",
    "########################################################################################################\n",
    "\n",
    "# Configuration for GPU\n",
    "@dataclass\n",
    "class TrainConfigGPU:\n",
    "    block_size: int = 256\n",
    "    batch_size: int = 64\n",
    "    max_iters: int = 2000\n",
    "    eval_interval: int = 250\n",
    "    eval_iters: int = 200\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 0.1\n",
    "    device: str = \"cuda\"\n",
    "\n",
    "# Configuration for CPU\n",
    "@dataclass\n",
    "class TrainConfigCPU:\n",
    "    block_size: int = 64\n",
    "    batch_size: int = 12\n",
    "    max_iters: int = 2000\n",
    "    eval_interval: int = 200\n",
    "    eval_iters: int = 50\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 0.1\n",
    "    device: str = \"cpu\"\n",
    "    compile: bool = False\n",
    "\n",
    "\n",
    "# Automatically select suitable configuration based on GPU detection\n",
    "if torch.cuda.is_available():\n",
    "  ModelConfig = ModelConfigGPU\n",
    "  TrainConfig = TrainConfigGPU\n",
    "else:\n",
    "  ModelConfig = ModelConfigCPU\n",
    "  TrainConfig = TrainConfigCPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6914a8f7",
   "metadata": {
    "id": "6914a8f7"
   },
   "source": [
    "### Building the NanoGPT Model\n",
    "\n",
    "The model begins with token embeddings, which map character indices to dense vectors, and positional embeddings, which encode the order of tokens within a fixed context window (`block_size`). These embeddings are added together and passed through a stack of transformer decoder blocks.\n",
    "\n",
    "Each decoder block consists of two main submodules: causal self-attention and a feed-forward network (MLP). Causal self-attention allows each token to attend only to previous tokens by applying a causal mask, ensuring that the model cannot access future information during training or generation. Multiple attention heads are used so that the model can focus on different aspects of the context in parallel. The MLP applies a non-linear transformation independently at each position, complementing the attention mechanism. Layer normalization and residual connections are used throughout to stabilize training and preserve information flow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GQjzz6O9zVb2",
   "metadata": {
    "id": "GQjzz6O9zVb2"
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, c: ModelConfig):\n",
    "        super().__init__()\n",
    "        assert c.n_embd % c.n_head == 0\n",
    "        self.n_head = c.n_head\n",
    "        self.head_dim = c.n_embd // c.n_head\n",
    "\n",
    "        self.qkv = nn.Linear(c.n_embd, 3 * c.n_embd, bias=False)\n",
    "        self.proj = nn.Linear(c.n_embd, c.n_embd, bias=False)\n",
    "        self.dropout = nn.Dropout(c.dropout)\n",
    "\n",
    "        mask = torch.tril(torch.ones(c.block_size, c.block_size)).view(1, 1, c.block_size, c.block_size)\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.split(C, dim=2)\n",
    "\n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.proj(y)\n",
    "        y = self.dropout(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, c: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(c.n_embd, 4 * c.n_embd)\n",
    "        self.proj = nn.Linear(4 * c.n_embd, c.n_embd)\n",
    "        self.dropout = nn.Dropout(c.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, c: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(c.n_embd)\n",
    "        self.attn = CausalSelfAttention(c)\n",
    "        self.ln2 = nn.LayerNorm(c.n_embd)\n",
    "        self.mlp = MLP(c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cgvSRhqr1Ltu",
   "metadata": {
    "id": "cgvSRhqr1Ltu"
   },
   "source": [
    "After passing through all decoder blocks, the model applies a final layer normalization and a linear output head that produces a probability distribution over the vocabulary for the next-token prediction. The training objective is standard cross-entropy loss between the predicted next token and the true next token.\n",
    "\n",
    "This NanoGPT model contains approximately between 0.8 and 3 million parameters, depending on the configuration. By comparison, GPT-2 contains hundreds of millions of parameters and modern commercial language models contain many billion parameters. Despite this difference in scale, the same architectural principles apply. The total number of parameters is mainly determined by the embedding dimension (`n_embd`), the number of layers (`n_layer`), and the vocabulary size, while the number of attention heads controls how the embedding space is partitioned rather than its overall size.\n",
    "\n",
    "The model is intentionally kept small so that it can be trained quickly on limited hardware and easily inspected. The goal of this assignment is to understand how transformer components interact, not to achieve state-of-the-art language modeling performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6762045",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T18:03:04.558560Z",
     "start_time": "2025-12-27T18:03:04.244986Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6762045",
    "outputId": "25cfabc8-6073-4022-b3b1-85b3ce01d555"
   },
   "outputs": [],
   "source": [
    "class NanoGPT(nn.Module):\n",
    "    def __init__(self, c: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.c = c\n",
    "        self.tok_emb = nn.Embedding(c.vocab_size, c.n_embd)\n",
    "        self.pos_emb = nn.Embedding(c.block_size, c.n_embd)\n",
    "        self.drop = nn.Dropout(c.dropout)\n",
    "        self.blocks = nn.ModuleList([Block(c) for _ in range(c.n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(c.n_embd)\n",
    "        self.head = nn.Linear(c.n_embd, c.vocab_size, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.c.block_size\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
    "\n",
    "        x = self.tok_emb(idx) + self.pos_emb(pos)\n",
    "        x = self.drop(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "cfg = TrainConfig()\n",
    "mcfg = ModelConfig(vocab_size=vocab_size, block_size=cfg.block_size)\n",
    "\n",
    "model = NanoGPT(mcfg).to(cfg.device)\n",
    "print(\"Parameters:\", sum(p.numel() for p in model.parameters())/1e6, \"M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ec49d",
   "metadata": {
    "id": "3a8ec49d"
   },
   "source": [
    "### The Training Loop\n",
    "\n",
    "We use the AdamW optimizer and periodically evaluate on the validation set. The training in Google Colab should take for both the CPU and GPU configurations approximately 4-5 minutes using the baseline configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd3b1d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T18:04:53.807905Z",
     "start_time": "2025-12-27T18:03:20.624644Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d5f46a7b70a24032bdd01c079189baf4",
      "2aa1f5941b934109ad203c8c2c42dd49",
      "79e8aa1ee73d4befb2a986d99946e8ad",
      "13c646d58aac4b989f228a491bdfde18",
      "71c88adb4d7f48fa860688a28eb3034c",
      "d8088cf79e3f490dbd46664b44c95a92",
      "72b4074f5a1c4010b8978b58bb874f85",
      "0e7826b4890f46e1a75f03737dd998bf",
      "3dc5a25fae164f5897affe5ec41bffc6",
      "785c36956f9a46dca5e5b9fd885de89f",
      "1b3fe486be924068a2bb98eea91dc15c"
     ]
    },
    "id": "4dd3b1d8",
    "outputId": "a37a3ec5-eea6-42eb-e274-c0058a4c93f9"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(cfg.eval_iters)\n",
    "        for k in range(cfg.eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "model.train()\n",
    "pbar = tqdm(range(cfg.max_iters), desc=\"training\")\n",
    "for it in pbar:\n",
    "    if it % cfg.eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        pbar.set_postfix(train=losses[\"train\"], val=losses[\"val\"])\n",
    "\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    _, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6af6d0",
   "metadata": {
    "id": "3b6af6d0"
   },
   "source": [
    "### Text Generation (Sampling)\n",
    "\n",
    "Now that the model is trained we can put it to use. We generate the synthetic captions starting from `\"H&E stained section showing\"` string by autoregressively sampling next characters\n",
    "\n",
    "Hyperparameters:\n",
    "- `temperature`: higher = more random, lower = less variety\n",
    "- `top_k`: restrict sampling to top-k most likely chars (lower number eliminates less likely candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7856711",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7856711",
    "outputId": "13c1a5c3-7c66-4839-dcea-4e5a0ca97ae2"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(start: str, max_new_tokens=400, temperature=1.0, top_k=60):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([encode(start)], dtype=torch.long, device=cfg.device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -cfg.block_size:]\n",
    "        logits, _ = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
    "\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, k=min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    return decode(idx[0].tolist())\n",
    "\n",
    "prompt = \"H&E stained section showing\"\n",
    "\n",
    "print(generate(prompt, max_new_tokens=500, temperature=0.7, top_k=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8m0HEEYvdI9n",
   "metadata": {
    "id": "8m0HEEYvdI9n"
   },
   "source": [
    "⚠️ *Everything below this line must be submitted as a deliverable for this assignment.*\n",
    "\n",
    "ℹ️ *Answering the exercises below will require you to implement new code and/or modify the code in cells above. You can add the code directly in the notebook or in separate Python files, depending on your preference. If you write code in separate files, please do not forget to also submit them. The exercises will also require re-running the training, sometimes multiple times. You can split the workload among the group members so things are done more efficiently.*\n",
    "\n",
    "## Exercises: Practice\n",
    "\n",
    "#### Exercise 1 \n",
    "\n",
    "When we created the text corpus, we used the <ENDC> separator to mark the end of a caption. Why is the <ENDC> separator needed? What would happen if you use a model trained without such separator in practice? In order to investigate this, train a model without the <ENDC> separator and compare the results with the model trained with the <ENDC> separator when generating captions.\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Train the model and plot training loss and validation loss as a function of training iterations. Modify the following hyperparameters and observe the effect:\n",
    "- Learning rate\n",
    "- Batch size\n",
    "- Evaluation interval\n",
    "\n",
    "Explain why the observed changes occur. What patterns indicate underfitting? What patterns suggest overfitting or unstable training?\n",
    "\n",
    "#### Exercise 3\n",
    "\n",
    "We can reduce the number of tokens by converting all letters to either uppercase or lowercase. Implement this in the preprocessing function and retrain a model. Report your observations on the training and performance of the model.\n",
    "\n",
    "\n",
    "#### Exercise 4\n",
    "\n",
    "The dataset contains 2500 figure captions. After encoding and splitting out 10% of the tokens for validation, we are left with 874488 training tokens. Is this also the number of training samples used to train our NanoGPT model? If yes, explain why, if not try to estimate the actual number of training samples.\n",
    "\n",
    "#### Exercise 5\n",
    "\n",
    "Text generation depends strongly on the sampling hyperparameters.\n",
    "\n",
    "Generate captions using at least three different temperature values and two different top_k values. Include at least 10 generated examples per hyperparameter configuration in your report.\n",
    "\n",
    "For each configuration, comment on the following characteristics of the generated samples:\n",
    "  - Fluency and structure\n",
    "  - Repetition or degeneration\n",
    "  - Factual plausibility (even if the content is synthetic)\n",
    "\n",
    "Identify optimal configuration of parameters that balances coherence and diversity of the produced synthetic captions, and justify your choice.\n",
    "\n",
    "## Flipped Classroom Log\n",
    "\n",
    "ℹ️ *You have to fill this log for both flipped classroom sessions for this assignment. You only fill the log for your group, not together with the group that you interacted with.*\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Preparation\n",
    "* **Clients:** List specific theoretical or implementation (code) questions prepared before class.\n",
    "* **Consultants:** List papers, videos, or code documentation reviewed to prepare. Note, this is not limited to the material listed above, you can add any new material that you used or found useful. \n",
    "\n",
    "#### Peer Interaction\n",
    "* **Clients:** Summarize the solutions or explanations received.\n",
    "* **Consultants:** Summarize advice given and specific resources shared.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b3ac41",
   "metadata": {},
   "source": [
    "### Logs\n",
    "#### First Flipped Classroom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3b95de",
   "metadata": {},
   "source": [
    "ℹ️ *Write the log for your group in this cell. Should be in a narrative style, aim for a max. of 600 words.*\n",
    "\n",
    "**Role**: Client/Consultant\n",
    "\n",
    "**Description** of activities during flipped classroom:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa3daf",
   "metadata": {},
   "source": [
    "#### Second Flipped Classroom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f175ca",
   "metadata": {},
   "source": [
    "ℹ️ *Write the log for your group in this cell. Should be in a narrative style, aim for a max. of 600 words.*\n",
    "\n",
    "**Role**: Client/Consultant\n",
    "\n",
    "**Description** of activities during flipped classroom:"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
